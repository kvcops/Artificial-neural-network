import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error

class NeuralNetwork:
    """
    A class representing a simple Artificial Neural Network for predicting concrete compressive strength.
    """
    def __init__(self, input_size, hidden_size):
        """
        Initializes the neural network with random weights and biases.

        Args:
            input_size (int): Number of neurons in the input layer.
            hidden_size (int): Number of neurons in the hidden layer.
        """
        self.input_size = input_size
        self.hidden_size = hidden_size
        # Initialize synaptic weights (input to hidden layer), random in range (-1,1)
        self.weights_ih = np.random.uniform(-1, 1, (hidden_size, input_size))
        # Initialize threshold values (hidden layer), random in range (-1,1)
        self.bias_h = np.random.uniform(-1, 1, hidden_size)
        # Initialize synaptic weight between hidden and output layers, random in range(-1,1)
        self.weights_ho = np.random.uniform(-1, 1, hidden_size)
        # Initialize the threshold of output layer, random in range (-1,1)
        self.bias_o = np.random.uniform(-1, 1)
        self.alpha = 1  # shape parameter for sigmoid
        self.learning_rate = 0.02 # learning rate parameter to scale error modification in backward pass.

    def sigmoid(self, x):
        """
        Applies the sigmoid activation function element-wise.

        Args:
            x (np.ndarray): Input array.

        Returns:
            np.ndarray: Output array after applying sigmoid.
        """
        return 1 / (1 + np.exp(-self.alpha * x))

    def forward(self, inputs):
        """
        Performs a forward pass through the network.

        Args:
            inputs (np.ndarray): Array of input values (size: input_size).

        Returns:
            tuple: A tuple containing (1) output from hidden layer and (2) linear output before passing through output
                   activation function of the ouput node
        """

        # Hidden layer activation (includes bias/ threshold)
        hidden_input = np.dot(self.weights_ih , inputs) + self.bias_h
        # Pass input into sigmoid for each hidden layer
        hidden_output = self.sigmoid(hidden_input)

        # Output layer calculation (no activation function)
        output_layer_output =  np.dot(self.weights_ho , hidden_output ) + self.bias_o

        return hidden_output, output_layer_output

    def backward(self, inputs,  desired_output, hidden_output,  output_layer_output):
        """
        Performs the backward pass, computing gradients and updating weights

        Args:
            inputs (np.ndarray): Normalized array input for the corresponding training example.
            desired_output (float): The corresponding desired ouput.
            hidden_output (np.ndarray): Vector of output values for each hidden layer node, before passing activation function.
            output_layer_output (float): Normalized value from the output layer.
        """
        # Calculate output Layer Error based on mean squared
        output_error = desired_output - output_layer_output
        # Compute hidden layer node errors
        hidden_error =   output_error *  (hidden_output * (1 - hidden_output)  * self.weights_ho)
        # Compute weight update for output layer
        dw_output  =   self.learning_rate  * hidden_output * output_error

        # Update thresholds in out put node (single output)
        db_output =    self.learning_rate  *   output_error

        # Weight Update from Input to hidden layer
        dw_hidden =   np.outer (self.learning_rate * hidden_error ,inputs )

        # Update bias of hidden layer nodes
        db_hidden =   self.learning_rate * hidden_error

        # Update all trainable parameters
        self.bias_o  +=  db_output
        self.weights_ho  +=  dw_output
        self.weights_ih +=   dw_hidden
        self.bias_h  += db_hidden

    def train(self, training_data, desired_outputs,  iterations=12040):
        """
        Trains the neural network using the provided training data and desired outputs.

        Args:
            training_data (np.ndarray): Normalized array of training input data.
            desired_outputs (np.ndarray): Normalized array of training output data.
            iterations (int): Training loop (back propagation) counter. Default of 12040 is from paper.
        """
        for i in range (iterations): # start the learning iterations
            total_error= 0 # reset the loss count
            for inputs , desired_output  in zip (training_data, desired_outputs):  # iterates over the input datasets

                # feed forward calculation
                hidden_output, output_layer_output  = self.forward(inputs)

                # adjust parameters for feedforward with backward algorithm
                self.backward(inputs, desired_output ,hidden_output,  output_layer_output)

                # Calculates Mean Squared Error, accumulates after every training iteration
                total_error += (desired_output - output_layer_output) ** 2

            # record the Mean squared error
            avg_error = np.mean(total_error)
            if i % 1000 == 0:  # Print error every 1000 iterations to observe progress
                print(f"Iteration {i}, Error: {avg_error:.4f}")

    def predict(self, inputs):
        """
        Performs prediction on test samples for output.

        Args:
            inputs (np.ndarray): Array of input data for testing.

        Returns:
            np.ndarray: Array of predictions.
        """
        predictions = []
        for input_sample in inputs:
            _, output_layer_output  = self.forward(input_sample)
            predictions.append(output_layer_output)
        return np.array(predictions)

# Load the dataset from the Excel file
excel_file = 'your_dataset.xlsx'  # Replace with the actual name of your Excel file
df = pd.read_excel(excel_file)

# --- Data Preprocessing ---
# 1. Separate input features and the target variable
X = df[['W/C RATIO', 'MIX PRAPOTIONS', 'CEMENT CONTENT(kg/m^3)', 'FLY ASH CONTENT(FA)kg/m^3']].copy()
y = df['28D(mpa)'].values

# 2. Extract the mix proportion values from the string format
def extract_proportions(mix_str):
    try:
        return list(map(float, mix_str.split(':')[1:]))
    except:
        return [np.nan, np.nan, np.nan]  # Handle potential errors

proportions = X['MIX PRAPOTIONS'].apply(extract_proportions)
X[['Cement_Prop', 'FA_Prop', 'Sand_Prop']] = proportions.to_list()
X.drop('MIX PRAPOTIONS', axis=1, inplace=True)

# Handle NaN values if introduced during proportion extraction
X.fillna(X.mean(), inplace=True)

# Convert to NumPy arrays
X = X.values
y = y.reshape(-1, 1) # Reshape y for consistent scaling

# 3. Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Feature Scaling (Normalization)
input_scaler = MinMaxScaler()
X_train_scaled = input_scaler.fit_transform(X_train)
X_test_scaled = input_scaler.transform(X_test)

output_scaler = MinMaxScaler()
y_train_scaled = output_scaler.fit_transform(y_train)
y_test_scaled = output_scaler.transform(y_test)

# --- Model Training ---
input_size = X_train_scaled.shape[1]
hidden_size = 7  # As suggested in the paper
nn = NeuralNetwork(input_size, hidden_size)
nn.train(X_train_scaled, y_train_scaled.flatten())

# --- Model Evaluation ---
y_train_pred_scaled = nn.predict(X_train_scaled)
y_test_pred_scaled = nn.predict(X_test_scaled)

# Unscale the predictions to evaluate in the original scale
y_train_pred = output_scaler.inverse_transform(y_train_pred_scaled.reshape(-1, 1))
y_test_pred = output_scaler.inverse_transform(y_test_pred_scaled.reshape(-1, 1))

# Calculate Mean Squared Error
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Train Mean Squared Error: {train_mse:.2f}")
print(f"Test Mean Squared Error: {test_mse:.2f}")

# --- Prediction on New Data ---
# Example of new data (should have the same number of features as the training data)
new_data = np.array([[0.48, 400, 0, 1.15, 1.30, 2.70]]) # Example with unscaled data
# Ensure the order of features matches the training data:
# 'W/C RATIO', 'CEMENT CONTENT(kg/m^3)', 'FLY ASH CONTENT(FA)kg/m^3', 'Cement_Prop', 'FA_Prop', 'Sand_Prop'

# You'll need to preprocess the new data similar to the training data
new_data_df = pd.DataFrame(new_data, columns=['W/C RATIO', 'CEMENT CONTENT(kg/m^3)', 'FLY ASH CONTENT(FA)kg/m^3', 'Cement_Prop', 'FA_Prop', 'Sand_Prop'])

# Scale the new data using the same scaler fitted on the training data
new_data_scaled = input_scaler.transform(new_data_df)

# Make the prediction
predicted_strength_scaled = nn.predict(new_data_scaled)

# Unscale the prediction
predicted_strength = output_scaler.inverse_transform(predicted_strength_scaled.reshape(-1, 1))

print("Predicted compressive strength for new data:", predicted_strength[0][0])
